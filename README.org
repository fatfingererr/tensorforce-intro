#+TITLE: TensorForce 簡介
#+SUBTITLE: 深度增強學習的好用庫
#+DATE: 2018/02/02 (四)
#+AUTHOR: fatfingererr @ Sukki 2018 二月聚會
#+EMAIL: fatfingererr@gmail.com
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:nil p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:nil todo:t |:t
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

#+GOOGLE_PLUS: https://plus.google.com/fatfingererr
#+WWW: http://fatfingererr.github.io/
#+GITHUB: http://github.com/fatfingererr
#+TWITTER: fatfingererr

#+FAVICON: images/sukki-icon.png
#+ICON: images/sukki-icon.png
#+HASHTAG: tensorforce

* TensorForce 基本介紹
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:

*** TensorForce 簡介

- 開源的 Reinforcement Learning (RL) library


- 建立在 TensorFlow 上，目前與 Python 2.7 和 3.5 相容


- 沒有對輸入和狀態有限制，可自由建構代理與環境


- 在 Agent Logic 和 Update Logic 做了嚴格分離，以便在真實環境中使用


- 主打將 RL Logic 呈現在 TensorFlow 的圖表上頭，減少對 Python 的依賴


*** 懶人安裝法

- 安裝詳細請見官網 [[https://github.com/reinforceio/tensorforce#installation][GitHub - reinforceio/tensorforce : Installation]]

#+BEGIN_SRC bash
# 已有 tensorflow
pip install tensorforce

# 沒有 tensorflow + 與 tensorflow 一同安裝
pip install tensorforce[tf]

# 務必更新, 否則可能會有 error
pip install --upgrade tensorforce
#+END_SRC


*** 開門見山來 DEMO

- 如果沒有 =gym= 請先安裝，詳細見官網 [[https://github.com/openai/gym#installation][GitHub - openai/gym]]

#+BEGIN_SRC bash
pip install gym
#+END_SRC

- 再來 demo ,  =TensorForce= 有提供 example

#+BEGIN_SRC bash
git clone https://github.com/reinforceio/tensorforce.git
cd tensorforce
#+END_SRC

*** 開門見山來 DEMO

- 測試執行請留意添加輸出資料夾的參數 =--monitor= ，方便檢視：


- 我則是在 tensorforce 資料夾底下開一個 =results= (windows)


#+BEGIN_SRC batch
mkdir results
python examples/openai_gym.py CartPole-v0^
                           -a examples/configs/vpg.json^
                           -n examples/configs/mlp2_network.json^
                           -e 100^
                           -m 5000^
                           --monitor results
#+END_SRC

*** DEMO 結果

- 輸出會有兩個 =json= 檔案

#+BEGIN_CENTER
#+ATTR_HTML: :width 800px
[[file:images/tensorforce-results.png]]
#+END_CENTER

*** DEMO 結果

- =epsiode_batch.json= 內容大致如下

#+BEGIN_SRC json
{"initial_reset_timestamp": 1517301297.9757686,

 "timestamps": [1517301297.9968548, 1517301298.0198836, ... , 1517301299.1702468],

 "episode_lengths": [14, 22, 29, 16, 15, ... , 28, 22, 24, 17, 21, 23],

 "episode_rewards": [14.0, 22.0, 29.0, 16.0, ..., 21.0, 24.0, 40.0, 23.0],

 "episode_types": ["t", "t", "t", "t", ... , "t", "t", "t", "t"]}
#+END_SRC


* TensorForce 流程說明
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:

*** TensorForce 流程

- TensorForce 的執行流程非常簡單


- 是在 TensorFlow 和 Oepn AI Gym 之間的連結橋梁


- OpenAI Gym 提供 =Environment= 也就是 =gym_id=


- 而 =TensorFlow= 被定義成 =Model= 讓 =Agent= 來初始化


- 透過建立一個集成物件 =Runner= 來進行 RL 訓練


*** TensorForce 流程

#+BEGIN_CENTER
#+ATTR_HTML: :width 900px
[[file:images/tensorforce-intro.png]]
#+END_CENTER


*** TensorForce 流程 - 以 openai-gym.py 為例 (1/7)

- 我們可以讀 code : [[https://github.com/reinforceio/tensorforce/blob/master/examples/openai_gym.py][tensorforce/openai-gym.py - GitHub]]

#+BEGIN_SRC python
import argparse # 1. 為了在 command-line 搭配不同 arg 執行
import json     # 2. 讀取 Agent 以及 Network 設置
import logging  # 3. 每個 epsiode 進行紀錄
import os       # 4. 操作檔案路徑
import time     # 5. 操作運算時間

from tensorforce import TensorForceError # 丟 Error
from tensorforce.agents import Agent     # 建立 Agent
from tensorforce.execution import Runner # 建立 Runner
from tensorforce.contrib.openai_gym import OpenAIGym # 建立 Env
#+END_SRC

*** TensorForce 流程 - 以 openai-gym.py 為例 (2/7)

- =monitor= 相關參數主要是為了 OpenAI Gym 的設置


- 建議不要輸出 Video ，可能遇到不知名錯誤而停止 (GUI Window 問題)

#+BEGIN_SRC python
# 建立環境 Environment
 environment = OpenAIGym(
        gym_id=args.gym_id,               # Gym ID 就是你的特定環境
        monitor=args.monitor,             # 是否要輸出 Gym Results
        monitor_safe=args.monitor_safe,   # 是否要避免蓋掉之前的 Results
        monitor_video=args.monitor_video  # 是否要每隔幾步輸出影片(危險!)
    )
#+END_SRC

*** TensorForce 流程 - 以 openai-gym.py 為例 (3/7)

- 接著是從 JSON 讀取相關設置， =spec= 結尾的函數是對 =dict= 字典資料的處理

#+BEGIN_SRC python
if args.agent_config is not None:
        <b>with open(args.agent_config, 'r') as fp:</b>
            <b>agent_config = json.load(fp=fp)</b>
    else:
        raise TensorForceError("No agent configuration provided.")

    if args.network_spec is not None:
        <b>with open(args.network_spec, 'r') as fp:</b>
            <b>network_spec = json.load(fp=fp)</b>
    else:
        network_spec = None
        logger.info("No network configuration provided.")
#+END_SRC

*** TensorForce 流程 - 以 openai-gym.py 為例 (4/7)

- 其中 Agent Config 的 JSON 大概長成這樣：

#+BEGIN_SRC json
{
    "type": "vpg_agent", # RL Agent 名稱
    "batch_size": 4000,        # TensorFlow 中的 batch size
    "optimizer": {             # TensorFlow 中的 optimizer
        "type": "adam",        # TensorFlow 中的 optimize type
        "learning_rate": 1e-2  # TensorFlow 中的 learning rate
    },
    "discount": 0.99,          # TensorFlow 中的 discount factor
    "entropy_regularization": null, # TensorFlow 中的... (略)
    # ...(略)
}
#+END_SRC

- 總之這裡的設置都是 TensorFlow 的基本參數設置


*** TensorForce 流程 - 以 openai-gym.py 為例 (5/7)

- 而 =networ_spec= 讀取的 JSON 就是 TensorFlow Model 的設置：

#+BEGIN_SRC json
{
    "type": "conv2d", "size": 32, "window": 8, "stride": 4
},
... (略)
{
    "type": "flatten"
},
{
    "type": "dense", "size": 512
}
#+END_SRC

*** TensorForce 流程 - 以 openai-gym.py 為例 (6/7)

- 接著你需要定義一個 =epsiode_finished= 後面講 runner 會提到


- 主要是方便你可以在每個 epsiode 使用 =logger= 輸出迭代資訊

#+BEGIN_SRC python
def episode_finished(r):
        if r.episode % report_episodes == 0:
            steps_per_second = r.timestep / (time.time() - r.start_time)
            logger.info("Finished episode {} after {} timesteps. Steps Per Second {}"
                        .format(r.agent.episode, r.episode_timestep, steps_per_second
            ))
        return True
#+END_SRC


*** TensorForce 流程 - 以 openai-gym.py 為例 (7/7)

- 最後就是執行 =rnuner.run= 即可，並且搭配 =close= 完成整個流程

#+BEGIN_SRC python
runner.run(
        timesteps=args.timesteps,
        episodes=args.episodes,
        max_episode_timesteps=args.max_episode_timesteps,
        deterministic=args.deterministic,
        episode_finished=episode_finished
    )
    runner.close()
#+END_SRC


* TensorForce 深入剖析
  :PROPERTIES:
  :SLIDE:    segue dark quote
  :ASIDE:    right bottom
  :ARTICLE:  flexbox vleft auto-fadein
  :END:

*** Agent 以及 Model 的設置

- 在 TensorForce 的 RL 框架中，有兩個東西要留意，分別是 =Runner= 以及 =Model=


- Agent (代理) 並不是直接與 Environment (環境) 交互，是透過 Runner


- Agent (代理) 可以有多個 Model ，例如 =Double-DQN= 就有兩個 =Q-Model=


#+BEGIN_CENTER
#+ATTR_HTML: :width 800px
[[file:images/agent-and-model-view-in-tensorforce.png]]
#+END_CENTER

*** Agent 類

- Agent 是一個 class ，所有的 Agent 都繼承自這個 Class


- 在 TensorForce 中，大部分的 Agent 是指一種 RL 方法，例如 =DQNAgent=


- 有些 Agent 要使用 Model 的歷史資訊（例如 RNN ）則要繼承自 =MemoryAgent=


- 有些 Agent 是在 Model 的每個 Batch 做 Replay 則要繼承自 =BatchAgent=


*** Agent 類
#+BEGIN_CENTER
#+ATTR_HTML: :width 800px
[[file:images/agent-class.png]]
#+END_CENTER


*** Agent 類 - 以 DQNAgent 為例

- =Agent= 本身主要放參數，詳細請見 [[https://github.com/reinforceio/tensorforce/blob/master/tensorforce/agents/dqn_agent.py][DQNAgent.py - GitHub]]

#+BEGIN_SRC python
class DQNAgent(MemoryAgent):
    def __init__(
　　　
      　# Agent 的參數
        self, states_spec,  actions_spec, batched_observe=None, scope='dqn', ...

        # Learning 的參數
        summary_spec=None, network_spec=None, device=None, ...

　　　  # DQNAgent 的特殊參數
        target_sync_frequency=10000, target_update_weight=1.0,
        double_q_model=False, huber_loss=None, ...
#+END_SRC


*** Agent 類 - 以 DQNAgent 為例

- 透過 =Agent= 來可以定義 Model 初始化函數 =initialize_model=

#+BEGIN_SRC python
def initialize_model(self):
        return QModel(
            states_spec=self.states_spec,
            actions_spec=self.actions_spec,
            network_spec=self.network_spec,
            ...
            double_q_model=self.double_q_model,
            huber_loss=self.huber_loss,
            random_sampling_fix=True
            )
#+END_SRC

* Thank You !
:PROPERTIES:
:SLIDE: thank-you-slide segue
:ASIDE: right
:ARTICLE: flexbox vleft auto-fadein
:END:

本投影片除引用之版權內容外，皆以 [[https://zh.wikipedia.org/wiki/WTFPL][WTFPL]] 釋出
